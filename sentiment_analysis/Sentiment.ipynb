{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Movie Review Dataset\n",
    "\n",
    "The movie data is a collection of movie reviews retreived from the IMDB.com websit in the early 2000s by Bo Pang and Lee. The reviews were collected and made available as part of their research on NLP.\n",
    "\n",
    "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at imdb.com. The authors refer to this dataset as the “polarity dataset.\n",
    "\n",
    "\n",
    "\n",
    "Download data from here: https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory set up\n",
    "- texttoken\n",
    "- - pos (Contains the postive review in txt format)\n",
    "- - neg (Contains the negative review in txt format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, \"r\")\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into token by white space\n",
    "    tokens =doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
    "\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "\n",
    "    tokens =  [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # filter out the stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # filter out the short tokens\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_trian:bool):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('cv9'): # only consdering first 900 data for trainig\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t\n",
    "\t\tadd_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "process_docs('txt_sentoken/neg', vocab, True)\n",
    "process_docs('txt_sentoken/pos', vocab, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common 50 words from this dataset of IMBD reviews\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most Common 50 words from this dataset of IMBD reviews\")\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25767"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep tokens with a min occurrece\n",
    "\n",
    "main_occurance =2\n",
    "tokens = [k for k,c in vocab.items() if c >= main_occurance]\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'church',\n",
       " 'party',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'get',\n",
       " 'accident',\n",
       " 'one',\n",
       " 'guys',\n",
       " 'dies',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'see',\n",
       " 'life',\n",
       " 'nightmares',\n",
       " 'whats',\n",
       " 'deal',\n",
       " 'watch',\n",
       " 'movie',\n",
       " 'sorta',\n",
       " 'find',\n",
       " 'critique',\n",
       " 'mindfuck',\n",
       " 'generation',\n",
       " 'touches',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'presents',\n",
       " 'bad',\n",
       " 'package',\n",
       " 'makes',\n",
       " 'review',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'write',\n",
       " 'since',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'attempt',\n",
       " 'break',\n",
       " 'mold',\n",
       " 'mess',\n",
       " 'head',\n",
       " 'lost',\n",
       " 'highway',\n",
       " 'memento',\n",
       " 'good',\n",
       " 'ways',\n",
       " 'making',\n",
       " 'types',\n",
       " 'folks',\n",
       " 'didnt',\n",
       " 'snag',\n",
       " 'correctly',\n",
       " 'seem',\n",
       " 'taken',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " 'executed',\n",
       " 'terribly',\n",
       " 'problems',\n",
       " 'well',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'simply',\n",
       " 'jumbled',\n",
       " 'starts',\n",
       " 'normal',\n",
       " 'downshifts',\n",
       " 'fantasy',\n",
       " 'world',\n",
       " 'audience',\n",
       " 'member',\n",
       " 'going',\n",
       " 'dreams',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'dead',\n",
       " 'others',\n",
       " 'look',\n",
       " 'like',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " 'disappearances',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " 'tons',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'happen',\n",
       " 'explained',\n",
       " 'personally',\n",
       " 'dont',\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'unravel',\n",
       " 'film',\n",
       " 'every',\n",
       " 'give',\n",
       " 'clue',\n",
       " 'kind',\n",
       " 'fed',\n",
       " 'biggest',\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'hide',\n",
       " 'seems',\n",
       " 'want',\n",
       " 'completely',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " 'make',\n",
       " 'entertaining',\n",
       " 'thrilling',\n",
       " 'engaging',\n",
       " 'meantime',\n",
       " 'really',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'arrow',\n",
       " 'dig',\n",
       " 'flicks',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'halfway',\n",
       " 'point',\n",
       " 'strangeness',\n",
       " 'start',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'sense',\n",
       " 'still',\n",
       " 'guess',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movies',\n",
       " 'always',\n",
       " 'sure',\n",
       " 'given',\n",
       " 'password',\n",
       " 'enter',\n",
       " 'understanding',\n",
       " 'mean',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'visions',\n",
       " 'throughout',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " 'okay',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'know',\n",
       " 'need',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'offering',\n",
       " 'insight',\n",
       " 'apparently',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'director',\n",
       " 'chopped',\n",
       " 'shows',\n",
       " 'mightve',\n",
       " 'decent',\n",
       " 'somewhere',\n",
       " 'suits',\n",
       " 'decided',\n",
       " 'turning',\n",
       " 'music',\n",
       " 'video',\n",
       " 'edge',\n",
       " 'would',\n",
       " 'actors',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'seemed',\n",
       " 'playing',\n",
       " 'exact',\n",
       " 'character',\n",
       " 'american',\n",
       " 'beauty',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " 'kudos',\n",
       " 'holds',\n",
       " 'entire',\n",
       " 'feeling',\n",
       " 'unraveling',\n",
       " 'overall',\n",
       " 'doesnt',\n",
       " 'stick',\n",
       " 'entertain',\n",
       " 'confusing',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'feels',\n",
       " 'redundant',\n",
       " 'runtime',\n",
       " 'despite',\n",
       " 'ending',\n",
       " 'explanation',\n",
       " 'craziness',\n",
       " 'came',\n",
       " 'oh',\n",
       " 'way',\n",
       " 'horror',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " 'packaged',\n",
       " 'someone',\n",
       " 'assuming',\n",
       " 'genre',\n",
       " 'hot',\n",
       " 'kids',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'sitting',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'whatever',\n",
       " 'skip',\n",
       " 'wheres',\n",
       " 'joblo',\n",
       " 'nightmare',\n",
       " 'elm',\n",
       " 'street',\n",
       " 'blair',\n",
       " 'witch',\n",
       " 'crow',\n",
       " 'salvation',\n",
       " 'stir',\n",
       " 'echoes',\n",
       " 'happy',\n",
       " 'bastards',\n",
       " 'quick',\n",
       " 'damn',\n",
       " 'bug',\n",
       " 'starring',\n",
       " 'jamie',\n",
       " 'lee',\n",
       " 'curtis',\n",
       " 'another',\n",
       " 'baldwin',\n",
       " 'brother',\n",
       " 'william',\n",
       " 'time',\n",
       " 'story',\n",
       " 'regarding',\n",
       " 'crew',\n",
       " 'tugboat',\n",
       " 'comes',\n",
       " 'across',\n",
       " 'deserted',\n",
       " 'russian',\n",
       " 'tech',\n",
       " 'ship',\n",
       " 'kick',\n",
       " 'power',\n",
       " 'within',\n",
       " 'gore',\n",
       " 'bringing',\n",
       " 'action',\n",
       " 'sequences',\n",
       " 'virus',\n",
       " 'empty',\n",
       " 'flash',\n",
       " 'substance',\n",
       " 'middle',\n",
       " 'nowhere',\n",
       " 'origin',\n",
       " 'pink',\n",
       " 'flashy',\n",
       " 'thing',\n",
       " 'hit',\n",
       " 'mir',\n",
       " 'course',\n",
       " 'donald',\n",
       " 'sutherland',\n",
       " 'stumbling',\n",
       " 'around',\n",
       " 'drunkenly',\n",
       " 'hey',\n",
       " 'lets',\n",
       " 'robots',\n",
       " 'acting',\n",
       " 'average',\n",
       " 'likes',\n",
       " 'youre',\n",
       " 'likely',\n",
       " 'work',\n",
       " 'halloween',\n",
       " 'wasted',\n",
       " 'hes',\n",
       " 'real',\n",
       " 'star',\n",
       " 'stan',\n",
       " 'robot',\n",
       " 'design',\n",
       " 'cgi',\n",
       " 'occasional',\n",
       " 'shot',\n",
       " 'picking',\n",
       " 'someones',\n",
       " 'brain',\n",
       " 'body',\n",
       " 'parts',\n",
       " 'turn',\n",
       " 'heres',\n",
       " 'otherwise',\n",
       " 'much',\n",
       " 'sunken',\n",
       " 'jaded',\n",
       " 'viewer',\n",
       " 'thankful',\n",
       " 'invention',\n",
       " 'based',\n",
       " 'late',\n",
       " 'television',\n",
       " 'show',\n",
       " 'name',\n",
       " 'mod',\n",
       " 'squad',\n",
       " 'tells',\n",
       " 'tale',\n",
       " 'three',\n",
       " 'reformed',\n",
       " 'criminals',\n",
       " 'employ',\n",
       " 'police',\n",
       " 'undercover',\n",
       " 'however',\n",
       " 'wrong',\n",
       " 'evidence',\n",
       " 'gets',\n",
       " 'stolen',\n",
       " 'immediately',\n",
       " 'suspicion',\n",
       " 'ads',\n",
       " 'cuts',\n",
       " 'claire',\n",
       " 'danes',\n",
       " 'nice',\n",
       " 'hair',\n",
       " 'cute',\n",
       " 'outfits',\n",
       " 'car',\n",
       " 'chases',\n",
       " 'stuff',\n",
       " 'blowing',\n",
       " 'sounds',\n",
       " 'first',\n",
       " 'fifteen',\n",
       " 'quickly',\n",
       " 'becomes',\n",
       " 'apparent',\n",
       " 'certainly',\n",
       " 'slick',\n",
       " 'looking',\n",
       " 'complete',\n",
       " 'costumes',\n",
       " 'isnt',\n",
       " 'enough',\n",
       " 'best',\n",
       " 'described',\n",
       " 'cross',\n",
       " 'hourlong',\n",
       " 'cop',\n",
       " 'stretched',\n",
       " 'span',\n",
       " 'hour',\n",
       " 'half',\n",
       " 'single',\n",
       " 'clich',\n",
       " 'matter',\n",
       " 'elements',\n",
       " 'recycled',\n",
       " 'everything',\n",
       " 'weve',\n",
       " 'already',\n",
       " 'seen',\n",
       " 'nothing',\n",
       " 'spectacular',\n",
       " 'sometimes',\n",
       " 'bordering',\n",
       " 'wooden',\n",
       " 'omar',\n",
       " 'epps',\n",
       " 'deliver',\n",
       " 'lines',\n",
       " 'bored',\n",
       " 'transfers',\n",
       " 'onto',\n",
       " 'escape',\n",
       " 'relatively',\n",
       " 'unscathed',\n",
       " 'giovanni',\n",
       " 'ribisi',\n",
       " 'plays',\n",
       " 'resident',\n",
       " 'crazy',\n",
       " 'man',\n",
       " 'ultimately',\n",
       " 'worth',\n",
       " 'watching',\n",
       " 'unfortunately',\n",
       " 'save',\n",
       " 'convoluted',\n",
       " 'apart',\n",
       " 'occupying',\n",
       " 'screen',\n",
       " 'young',\n",
       " 'cast',\n",
       " 'clothes',\n",
       " 'hip',\n",
       " 'soundtrack',\n",
       " 'appears',\n",
       " 'geared',\n",
       " 'towards',\n",
       " 'teenage',\n",
       " 'mindset',\n",
       " 'rating',\n",
       " 'content',\n",
       " 'justify',\n",
       " 'juvenile',\n",
       " 'older',\n",
       " 'information',\n",
       " 'literally',\n",
       " 'spoonfed',\n",
       " 'hard',\n",
       " 'instead',\n",
       " 'telling',\n",
       " 'dialogue',\n",
       " 'poorly',\n",
       " 'written',\n",
       " 'extremely',\n",
       " 'predictable',\n",
       " 'progresses',\n",
       " 'wont',\n",
       " 'care',\n",
       " 'heroes',\n",
       " 'jeopardy',\n",
       " 'youll',\n",
       " 'arent',\n",
       " 'basing',\n",
       " 'nobody',\n",
       " 'remembers',\n",
       " 'questionable',\n",
       " 'wisdom',\n",
       " 'especially',\n",
       " 'considers',\n",
       " 'target',\n",
       " 'fact',\n",
       " 'number',\n",
       " 'memorable',\n",
       " 'counted',\n",
       " 'hand',\n",
       " 'thats',\n",
       " 'missing',\n",
       " 'finger',\n",
       " 'times',\n",
       " 'checked',\n",
       " 'six',\n",
       " 'clear',\n",
       " 'indication',\n",
       " 'cash',\n",
       " 'spending',\n",
       " 'dollar',\n",
       " 'judging',\n",
       " 'rash',\n",
       " 'awful',\n",
       " 'seeing',\n",
       " 'avoid',\n",
       " 'costs',\n",
       " 'quest',\n",
       " 'camelot',\n",
       " 'warner',\n",
       " 'bros',\n",
       " 'featurelength',\n",
       " 'steal',\n",
       " 'clout',\n",
       " 'disneys',\n",
       " 'cartoon',\n",
       " 'empire',\n",
       " 'mouse',\n",
       " 'reason',\n",
       " 'worried',\n",
       " 'recent',\n",
       " 'challenger',\n",
       " 'throne',\n",
       " 'last',\n",
       " 'falls',\n",
       " 'promising',\n",
       " 'flawed',\n",
       " 'century',\n",
       " 'fox',\n",
       " 'anastasia',\n",
       " 'hercules',\n",
       " 'lively',\n",
       " 'colorful',\n",
       " 'palate',\n",
       " 'beat',\n",
       " 'handsdown',\n",
       " 'crown',\n",
       " 'piece',\n",
       " 'animation',\n",
       " 'year',\n",
       " 'contest',\n",
       " 'arrival',\n",
       " 'magic',\n",
       " 'kingdom',\n",
       " 'mediocre',\n",
       " 'pocahontas',\n",
       " 'keeping',\n",
       " 'score',\n",
       " 'nearly',\n",
       " 'dull',\n",
       " 'revolves',\n",
       " 'adventures',\n",
       " 'freespirited',\n",
       " 'kayley',\n",
       " 'voiced',\n",
       " 'daughter',\n",
       " 'belated',\n",
       " 'knight',\n",
       " 'king',\n",
       " 'arthurs',\n",
       " 'round',\n",
       " 'table',\n",
       " 'dream',\n",
       " 'follow',\n",
       " 'fathers',\n",
       " 'footsteps',\n",
       " 'chance',\n",
       " 'evil',\n",
       " 'warlord',\n",
       " 'gary',\n",
       " 'oldman',\n",
       " 'steals',\n",
       " 'magical',\n",
       " 'sword',\n",
       " 'excalibur',\n",
       " 'accidentally',\n",
       " 'loses',\n",
       " 'dangerous',\n",
       " 'forest',\n",
       " 'help',\n",
       " 'hunky',\n",
       " 'blind',\n",
       " 'garrett',\n",
       " 'carey',\n",
       " 'elwes',\n",
       " 'twoheaded',\n",
       " 'dragon',\n",
       " 'eric',\n",
       " 'idle',\n",
       " 'rickles',\n",
       " 'arguing',\n",
       " 'might',\n",
       " 'able',\n",
       " 'medieval',\n",
       " 'sexist',\n",
       " 'prove',\n",
       " 'fighter',\n",
       " 'side',\n",
       " 'pure',\n",
       " 'showmanship',\n",
       " 'essential',\n",
       " 'element',\n",
       " 'expected',\n",
       " 'climb',\n",
       " 'high',\n",
       " 'ranks',\n",
       " 'disney',\n",
       " 'theres',\n",
       " 'differentiates',\n",
       " 'something',\n",
       " 'youd',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " 'subpar',\n",
       " 'instantly',\n",
       " 'forgettable',\n",
       " 'songs',\n",
       " 'computerized',\n",
       " 'footage',\n",
       " 'compare',\n",
       " 'runin',\n",
       " 'angry',\n",
       " 'ogre',\n",
       " 'battle',\n",
       " 'hydra',\n",
       " 'rest',\n",
       " 'case',\n",
       " 'stink',\n",
       " 'none',\n",
       " 'remotely',\n",
       " 'interesting',\n",
       " 'race',\n",
       " 'end',\n",
       " 'tie',\n",
       " 'win',\n",
       " 'dragons',\n",
       " 'comedy',\n",
       " 'shtick',\n",
       " 'awfully',\n",
       " 'cloying',\n",
       " 'least',\n",
       " 'signs',\n",
       " 'pulse',\n",
       " 'fans',\n",
       " 'tgif',\n",
       " 'lineup',\n",
       " 'thrilled',\n",
       " 'urkel',\n",
       " 'white',\n",
       " 'bronson',\n",
       " 'pinchot',\n",
       " 'sharing',\n",
       " 'nicely',\n",
       " 'realized',\n",
       " 'though',\n",
       " 'im',\n",
       " 'loss',\n",
       " 'recall',\n",
       " 'specific',\n",
       " 'providing',\n",
       " 'voice',\n",
       " 'talent',\n",
       " 'enthusiastic',\n",
       " 'paired',\n",
       " 'singers',\n",
       " 'sound',\n",
       " 'musical',\n",
       " 'moments',\n",
       " 'jane',\n",
       " 'seymour',\n",
       " 'celine',\n",
       " 'must',\n",
       " 'strain',\n",
       " 'aside',\n",
       " 'children',\n",
       " 'probably',\n",
       " 'adults',\n",
       " 'grievous',\n",
       " 'error',\n",
       " 'lack',\n",
       " 'personality',\n",
       " 'learn',\n",
       " 'goes',\n",
       " 'long',\n",
       " 'synopsis',\n",
       " 'mentally',\n",
       " 'unstable',\n",
       " 'undergoing',\n",
       " 'psychotherapy',\n",
       " 'saves',\n",
       " 'boy',\n",
       " 'potentially',\n",
       " 'fatal',\n",
       " 'love',\n",
       " 'boys',\n",
       " 'mother',\n",
       " 'fledgling',\n",
       " 'restauranteur',\n",
       " 'unsuccessfully',\n",
       " 'attempting',\n",
       " 'gain',\n",
       " 'womans',\n",
       " 'favor',\n",
       " 'takes',\n",
       " 'pictures',\n",
       " 'kills',\n",
       " 'comments',\n",
       " 'stalked',\n",
       " 'yet',\n",
       " 'seemingly',\n",
       " 'endless',\n",
       " 'string',\n",
       " 'type',\n",
       " 'stable',\n",
       " 'category',\n",
       " 'industry',\n",
       " 'theatrical',\n",
       " 'directtovideo',\n",
       " 'proliferation',\n",
       " 'may',\n",
       " 'due',\n",
       " 'theyre',\n",
       " 'typically',\n",
       " 'inexpensive',\n",
       " 'produce',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'stars',\n",
       " 'serve',\n",
       " 'vehicles',\n",
       " 'nudity',\n",
       " 'allowing',\n",
       " 'frequent',\n",
       " 'latenight',\n",
       " 'cable',\n",
       " 'slightly',\n",
       " 'norm',\n",
       " 'respect',\n",
       " 'psycho',\n",
       " 'never',\n",
       " 'affair',\n",
       " 'contrary',\n",
       " 'rejected',\n",
       " 'rather',\n",
       " 'exlover',\n",
       " 'exwife',\n",
       " 'exhusband',\n",
       " 'entry',\n",
       " 'doomed',\n",
       " 'collect',\n",
       " 'dust',\n",
       " 'viewed',\n",
       " 'midnight',\n",
       " 'provide',\n",
       " 'suspense',\n",
       " 'sets',\n",
       " 'interspersed',\n",
       " 'opening',\n",
       " 'credits',\n",
       " 'instance',\n",
       " 'narrator',\n",
       " 'spouts',\n",
       " 'stalkers',\n",
       " 'ponders',\n",
       " 'cause',\n",
       " 'stalk',\n",
       " 'implicitly',\n",
       " 'implied',\n",
       " 'men',\n",
       " 'shown',\n",
       " 'actor',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'states',\n",
       " 'daryl',\n",
       " 'gleason',\n",
       " 'stalker',\n",
       " 'brooke',\n",
       " 'daniels',\n",
       " 'meant',\n",
       " 'called',\n",
       " 'required',\n",
       " 'proceeds',\n",
       " 'begins',\n",
       " 'obvious',\n",
       " 'sequence',\n",
       " 'contrived',\n",
       " 'quite',\n",
       " 'brings',\n",
       " 'victim',\n",
       " 'together',\n",
       " 'obsesses',\n",
       " 'follows',\n",
       " 'tries',\n",
       " 'woo',\n",
       " 'plans',\n",
       " 'become',\n",
       " 'desperate',\n",
       " 'elaborate',\n",
       " 'include',\n",
       " 'alltime',\n",
       " 'cliche',\n",
       " 'murdered',\n",
       " 'pet',\n",
       " 'genres',\n",
       " 'require',\n",
       " 'found',\n",
       " 'exception',\n",
       " 'cat',\n",
       " 'shower',\n",
       " 'events',\n",
       " 'lead',\n",
       " 'inevitable',\n",
       " 'showdown',\n",
       " 'survives',\n",
       " 'invariably',\n",
       " 'conclusion',\n",
       " 'turkey',\n",
       " 'uniformly',\n",
       " 'adequate',\n",
       " 'anything',\n",
       " 'home',\n",
       " 'either',\n",
       " 'turns',\n",
       " 'toward',\n",
       " 'melodrama',\n",
       " 'overdoes',\n",
       " 'words',\n",
       " 'manages',\n",
       " 'creepy',\n",
       " 'pass',\n",
       " 'demands',\n",
       " 'dabo',\n",
       " 'close',\n",
       " 'played',\n",
       " 'bond',\n",
       " 'chick',\n",
       " 'living',\n",
       " 'daylights',\n",
       " 'equally',\n",
       " 'title',\n",
       " 'ditzy',\n",
       " 'strong',\n",
       " 'independent',\n",
       " 'needs',\n",
       " 'proceed',\n",
       " 'example',\n",
       " 'suspicions',\n",
       " 'ensure',\n",
       " 'use',\n",
       " 'excuse',\n",
       " 'decides',\n",
       " 'return',\n",
       " 'toolbox',\n",
       " 'left',\n",
       " 'place',\n",
       " 'house',\n",
       " 'leave',\n",
       " 'door',\n",
       " 'answers',\n",
       " 'opens',\n",
       " 'wanders',\n",
       " 'returns',\n",
       " 'enters',\n",
       " 'heroine',\n",
       " 'danger',\n",
       " 'somehow',\n",
       " 'parked',\n",
       " 'front',\n",
       " 'right',\n",
       " 'oblivious',\n",
       " 'presence',\n",
       " 'inside',\n",
       " 'whole',\n",
       " 'episode',\n",
       " 'places',\n",
       " 'incredible',\n",
       " 'audiences',\n",
       " 'suspension',\n",
       " 'disbelief',\n",
       " 'questions',\n",
       " 'validity',\n",
       " 'intelligence',\n",
       " 'receives',\n",
       " 'highly',\n",
       " 'derivative',\n",
       " 'somewhat',\n",
       " 'boring',\n",
       " 'cannot',\n",
       " 'watched',\n",
       " 'rated',\n",
       " 'mostly',\n",
       " 'several',\n",
       " 'murder',\n",
       " 'brief',\n",
       " 'strip',\n",
       " 'bar',\n",
       " 'offensive',\n",
       " 'many',\n",
       " 'thrillers',\n",
       " 'mood',\n",
       " 'stake',\n",
       " 'else',\n",
       " 'capsule',\n",
       " 'planet',\n",
       " 'mars',\n",
       " 'taking',\n",
       " 'custody',\n",
       " 'accused',\n",
       " 'murderer',\n",
       " 'face',\n",
       " 'menace',\n",
       " 'lot',\n",
       " 'fighting',\n",
       " 'john',\n",
       " 'carpenter',\n",
       " 'reprises',\n",
       " 'ideas',\n",
       " 'previous',\n",
       " 'assault',\n",
       " 'precinct',\n",
       " 'homage',\n",
       " 'believes',\n",
       " 'fight',\n",
       " 'horrible',\n",
       " 'writer',\n",
       " 'supposedly',\n",
       " 'expert',\n",
       " 'mistake',\n",
       " 'ghosts',\n",
       " 'drawn',\n",
       " 'humans',\n",
       " 'surprisingly',\n",
       " 'alien',\n",
       " 'addition',\n",
       " 'anybody',\n",
       " 'made',\n",
       " 'grounds',\n",
       " 'sue',\n",
       " 'chock',\n",
       " 'full',\n",
       " 'pieces',\n",
       " 'prince',\n",
       " 'darkness',\n",
       " 'surprising',\n",
       " 'managed',\n",
       " 'fit',\n",
       " 'admittedly',\n",
       " 'novel',\n",
       " 'science',\n",
       " 'fiction',\n",
       " 'experience',\n",
       " 'walk',\n",
       " 'surface',\n",
       " 'without',\n",
       " 'breathing',\n",
       " 'gear',\n",
       " 'budget',\n",
       " 'mentioned',\n",
       " 'gravity',\n",
       " 'increased',\n",
       " 'easier',\n",
       " 'society',\n",
       " 'changed',\n",
       " 'advanced',\n",
       " 'culture',\n",
       " 'women',\n",
       " 'positions',\n",
       " 'control',\n",
       " 'carpenters',\n",
       " 'view',\n",
       " 'female',\n",
       " 'beyond',\n",
       " 'minor',\n",
       " 'technological',\n",
       " 'advances',\n",
       " 'less',\n",
       " 'expect',\n",
       " 'change',\n",
       " 'ten',\n",
       " 'basic',\n",
       " 'common',\n",
       " 'except',\n",
       " 'yes',\n",
       " 'replaced',\n",
       " 'tacky',\n",
       " 'rundown',\n",
       " 'martian',\n",
       " 'mining',\n",
       " 'colony',\n",
       " 'criminal',\n",
       " 'wilson',\n",
       " 'desolation',\n",
       " 'williams',\n",
       " 'facing',\n",
       " 'hoodlums',\n",
       " 'automatic',\n",
       " 'weapons',\n",
       " 'nature',\n",
       " 'behave',\n",
       " 'manner',\n",
       " 'essentially',\n",
       " 'human',\n",
       " 'savages',\n",
       " 'lapse',\n",
       " 'imagination',\n",
       " 'told',\n",
       " 'flashback',\n",
       " 'entirely',\n",
       " 'night',\n",
       " 'filmed',\n",
       " 'almost',\n",
       " 'tones',\n",
       " 'red',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list(tokens, 'vocab.txt') # We will use this later for training the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update clean_doc \n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update process_docs\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    document=[]\n",
    "\n",
    "    for filename in listdir(directory):\n",
    "        if is_trian and filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        path = directory + \"/\" + filename\n",
    "\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "\n",
    "        document.append(tokens)\n",
    "        return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector representation within the embedding. These vectors are random at the beginning of training, but during training become meaningful to the network.**\n",
    "\n",
    "\n",
    "We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also need to ensure that all documents have the same length.\n",
    "\n",
    "This is a requirement of Keras for efficient computation. We could truncate reviews to the smallest size or zero-pad (pad with the value ‘0’) reviews to the maximum length, or some hybrid. In this case, we will pad all reviews to the length of the longest review in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad sequence\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training labels (This may not work if you load and add negative docs first)\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test reviews\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "#max_length = max([len(s.split()) for s in train_docs])\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now ready to define our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector space \n",
    "vec_space = 100  # One can use 50 , 150 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding NN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    100,\n",
    "                    input_length=max_length\n",
    "                    )) \n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation=\"relu\"))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 387, 100)          54000     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 380, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 190, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6080)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                60810     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,453\n",
      "Trainable params: 140,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a binary cross entropy loss function because the problem we are learning is a binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile network\n",
    "model.compile(loss= \"binary_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= Xtrain\n",
    "y= ytrain.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= x.reshape(2,387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 774)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 900)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 900)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24736/1544026860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\temp\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 900)).\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "\n",
    "model.fit(Xtrain, y, epochs=10, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b2624bcefef9b1a8e3df95e751ab7daeecd17b4193542664715a51e1b0603ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
